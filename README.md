# third_hw_de_course2025_misis
   Третье ДЗ на курсе по Инженирингу данных от ITam | ETL, Airflow, Spark

# Домашнее Задание 3 : Работа по проектированию и разработке ETL процесса 

## Датасет:

- Датасет лежит в kaggle - [[Ссылка](https://www.kaggle.com/datasets/justinwilcher/nashville-accident-reports-jan-2018-apl-2025)] 

Есть четкое описание, как можно его скачать, достаточно перейти по ссылке выше

## Общее:

В данной работе мы с вами создадим свой первый полноценный ETL процесс. Используем все технологии, которые были разобраны на карсе за весь период обучения. 

## Технологический стек, необходимый для исползования в данном ДЗ:

Apache Airflow - оркестратор для ETL. В ДЗ рекомендуем использовать python операторы

Pyspark (Spark) - фреймворк для работы с данными методами распределенных вычислений. В данной работе считывать данные, трансформить их и выгрзужать в другие системы необходимо спарком

Postgres - выступает в роли OLTP системы для хранения данных 

Hadoop, HDFS, Hive - для кластерных вычислений

## Описание задания для постановки на регламент при помощи airflow:

### Текст задания: 

Необходимо написать регламентный процесс, который будет наполнять таблицы hive 1 раз в месяц данными за предыдущий месяц. Расписание можно поставить на любой день месяца, не принципиально. (Пример: даг бежит 3его числа каждого месяца: так, если процесс побежит 3 марта, то он посмотрит в файл и обработает данные за весь февраль, так как он будет являться отчетным периодом). Далее, после того, как таблицы в hive обогатились данными, необходимо реплицировать данные из hive в postgres. Важно учесть, что в hive у нас будут исторические таблицы (то есть хранием данные за все месяцы, за которые уже отбежал даг), а в Postgres мы будем хранить данные только за крайний месяц, остальные будем перетирать. Датасет содержит данные за январь 2018 г. - апрель 2025 г., Учтите это и тестируйте DAG на историческом периоде (установить start_date и catchup). Кол-во тасок в DAG может быть любым, главное обоснуйте, почему решили сделать именно так.

### Этапы разработки

1. Считываем csv-файл и записываем его в HDFS

2. Далее нам необходимо провести анализ датасета и создать три пользовательских таблиц на основе этих данных. Таблицы могут быть совершенно разные, но логичные. Эти таблицы необходимо записать в hive. Подсказка: при наполнении данными в коде таски дага либо создавайте новые партиции с данными помесячно, либо просто вставляйте новые данные, ПЕРЕСОЗДАВАТЬ ТАБЛИЦЫ В HIVE НЕ НУЖНО!

#### Spark-сессию создаем со следующей конфигурацией, необходимой для работы с hive

```
spark = SparkSession.builder \
        .appName("SparkHiveExample") \
        .enableHiveSupport() \ # to enable Hive support
        .getOrCreate()
```

#### Изначально в нотубке можем создать таблицу в hive, например таким образом
```
spark.sql("CREATE DATABASE IF NOT EXISTS test_db")

spark.sql("""
CREATE TABLE IF NOT EXISTS test_db.test_table (
    ProductId STRING,
    TotalReviews INT,
    AvgRating DOUBLE,
    FiveStarReviews INT # поля в примере случайные
)
STORED AS ORC
""")
```

3. Затем  данные из этих таблиц вычитываем при помощи spark и перекладываем в Postgres c помощью JDBC. Предудыщие данные за прошлый месяц в Postgres затираем. Здесь еобходимо почитать про то, как ходить в PG при помощи спарка через JDBC, почитать про JAR-файлы для работы с посгресом и обратить внимание на конфигурационные параметры Спарк-сессии.

PS: Важно учесть, что при разработке регламентного процесса какие-то действия необходимо сделвть до создания DAG. Так, загрузить csv файл в hdfs можно и нужно один раз через CLI, не нужно грузить датасет с kaggle каждую итерацию вашего дага.

## Формат сдачи

форк репозитория, код DAG и python таксок вашего DAG, функции, которые реализовали, все команлды hdfs, которые выполняли, если таковые имеются, текстовое подробное описание ваших действий и регламентного процесса, написанного вами. Удачи!


